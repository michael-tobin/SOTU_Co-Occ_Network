{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping word Co-occurences the 2022 'State of the Union' address\n",
    "In this notebook, we map the co-occurences of words within each sentence of the 2022 State Of the Union address. For each sentence, after removing stopwords, we log the occurences of each word with each other word in the sentence; these are the co-occurences. After doing this for each sentence, we can build a matrix of thes co-occurences, which can then be used to map edges (co-occurences) between nodes (words).\n",
    "\n",
    "\n",
    "One important convention if you are going to play around with this notebook:\n",
    "- Comments with # are optional lines of code. To run that line, just remove the # and rerun the cell.  \n",
    "- Comments with ## are just comments and should be left as is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installs/ Downloads/ Imports\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installs\n",
    "- If you get install errors, run these commands in a terminal window instead\n",
    "- These are only needed if you do not already have these packages installed\n",
    "- Only need to run the first time you run the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For building and analyzing the network graph\n",
    "#pip install networkx  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For tokenizing the text and stopwords\n",
    "#pip install nltk  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For visualizing the graph\n",
    "#pip install pyvis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk ## Natural Language Tool Kit must be imported for the below downloads to run\n",
    "from nltk.tokenize import word_tokenize ## One available tokenizer in NLTK\n",
    "from nltk.corpus import stopwords ## Stopwords are words that are common in many languages and are not important to the analysis\n",
    "import re ## Regex (Regular Expressions)\n",
    "import pandas as pd ## Pandas (Panel Data)\n",
    "import itertools ## For iterating over the data\n",
    "import networkx as nx ## NetworkX is a Python library for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks.\n",
    "from networkx.algorithms.community import greedy_modularity_communities ## For finding communities in the graph\n",
    "from pyvis.network import Network ## For visualizing the graph\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloads\n",
    "- Make sure that you have run the imports before running these\n",
    "- Only need to be run if you do not already have these downloaded\n",
    "- Only need to be run the first time you run this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords') # Dictionary of stopwords to be removed later\n",
    "# nltk.download('punkt') # For tokenizing the file into sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make some preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup stop words\n",
    "stops = set(stopwords.words('english'))\n",
    "#print (stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data from file\n",
    "We want to keep a copy of the original unaltered data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in file\n",
    "path = \"Data/Sotu22.txt\"\n",
    "with open(path, 'rb') as f:\n",
    "  contents = f.read()\n",
    "contents=contents.decode('utf-16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up the data\n",
    "We create two different versions of the data here:\n",
    "1. 'contents' will be used to split into sentences and get the co-occurences. For this purpose, we leave periods in to delineate sentences.\n",
    "\n",
    "2. 'cont_for_tok' or 'contents for tokens' will be used to create a set of all words in the SOTU. This will be used in the creation of the co-occurence matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'madam speaker madam vice president and our first lady and second gentleman. members of congress and the cabinet. justices of the supreme court. my fellow americans.  last year covid19 kept us apart. this year we are finally together again.  tonight w'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Remove punctuation, but leave periods to define sentences. \n",
    "## Make all words lowercase, otherwise the system will identify different cases as different words\n",
    "reg1 = re.compile('[^a-zA-Z\\s\\d\\.]')\n",
    "contents = re.sub(reg1, '', contents).lower()\n",
    "\n",
    "## Remove new line characters\n",
    "reg2 = r'\\n'\n",
    "contents = re.sub(reg2, ' ', contents)\n",
    "#contents[0:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split off a version of contents for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'madam speaker madam vice president and our first lady and second gentleman  members of congress and the cabinet  justices of the supreme court  my fellow americans   last year covid19 kept us apart  this year we are finally together again   tonight w'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Remove periods and create contents for tokens \n",
    "reg3 = r'\\.'\n",
    "cont_for_tok = re.sub(reg3, ' ', contents)\n",
    "cont_for_tok[0:250] ## Periods have been removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1860"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Tokenize cont_for_tok into individual words\n",
    "cont_for_tok = set(word_tokenize(cont_for_tok)) \n",
    "len(cont_for_tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Co-occurence Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create empty matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1747"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Iterate through the tokenized words and remove stop words\n",
    "wordsFiltered = []\n",
    "for word in cont_for_tok:\n",
    "    if word not in stops:\n",
    "        wordsFiltered.append(word)\n",
    "len(wordsFiltered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1748)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Columns\n",
    "matrix = pd.DataFrame(columns=wordsFiltered)\n",
    "\n",
    "## Add a row in the first position\n",
    "## This will hold our words for co-occurences\n",
    "## The words in this column will match the words in the column labels\n",
    "matrix.insert(0, 'co_occ', 0)\n",
    "matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1747, 1748)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create Rows. This one will take a while because it has to fill in all of the columns\n",
    "## Note that there is one more column than rows. This is because we added the column 'co_occ' to accomodate the row words\n",
    "for word in wordsFiltered:\n",
    "    matrix = matrix.append({'co_occ':word}, ignore_index=True).fillna(0)\n",
    "matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill the matrix with co-occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['madam speaker madam vice president and our first lady and second gentleman.',\n",
       " 'members of congress and the cabinet.',\n",
       " 'justices of the supreme court.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## break text into a list of strings (sentences)\n",
    "sentences = nltk.tokenize.sent_tokenize(contents)\n",
    "sentences[0:3] ## Notice that we still have periods. We don't want these in the coming steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['year finally together ',\n",
       " 'tonight meet democrats republicans independents ',\n",
       " 'importantly americans ',\n",
       " 'duty one another america american people constitution ',\n",
       " 'unwavering resolve freedom always triumph tyranny ']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## One more cleanup step, since we no longer need periods.\n",
    "## If the periods are left in, the system will think that a word... \n",
    "## with a period attached is different than the same word without one.\n",
    "sentences_filtered = []\n",
    "\n",
    "for sent in sentences:\n",
    "    temp_sent = ''\n",
    "    ## Replace the periods with spaces\n",
    "    ## We replace with a space because there are instances of 'word.word' in the text\n",
    "    sent = re.sub(reg3, ' ', sent) \n",
    "    ## Tokenize each sentence to make iterating their words easier\n",
    "    tokens = word_tokenize(sent)\n",
    "    ## If the word is not in the stopwords list, add it to the temporary sentence, separated by a space\n",
    "    for word in tokens:\n",
    "        if word not in stops:\n",
    "            temp_sent = temp_sent + word + ' '\n",
    "    ## Once asentence is complete, add it to the list of sentences.\n",
    "    ## This keeps the sentences separated instead of one long string\n",
    "    sentences_filtered.append(temp_sent.strip())\n",
    "sentences_filtered[5:10] ## Notice that all periods have been removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Iterate through each sentence\n",
    "## 2. compute the combinations of words within it\n",
    "## 3. Add the co-occurrence to the matrix\n",
    "for sent in sentences_filtered:\n",
    "    tokens = word_tokenize(sent)\n",
    "    for L in range(0, len(tokens)+1):\n",
    "        for subset in itertools.combinations(tokens, 2):\n",
    "            matrix.loc[matrix.co_occ == subset[0], subset[1]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The matrix is too large to view here, but opening the CSV shows that we were successful\n",
    "matrix.to_csv('Data/SOTU_Matrix.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a list of edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a new dataframe to hold the edges\n",
    "## The columns will take the x and y coordinates of the matrix as source and target\n",
    "## and the value at their intersection as the weight\n",
    "edges = pd.DataFrame(columns=['source', 'target', 'weight'])\n",
    "\n",
    "for row in matrix.iterrows():\n",
    "    for col in matrix.columns:\n",
    "        ## Ignore the first column as that is the same as the source\n",
    "        ## Without this we would end up with self referential rows\n",
    "        if col == 'co_occ':\n",
    "            pass\n",
    "        ## If the value is greater than 0, then we add it to the edges dataframe\n",
    "        ## This significantly cuts down on the size of the dataframe\n",
    "        else:\n",
    "            if matrix.at[row[0], col] != 0.0:\n",
    "                new_row = {'source':matrix.at[row[0], 'co_occ'],\n",
    "                        'target':col,\n",
    "                        'weight':(matrix.at[row[0], col])/2} ## Cut the weight in half to account for reciprocal edges\n",
    "                edges = edges.append(new_row, ignore_index=True)\n",
    "            else:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges.to_csv('Data/edges.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize the graph from the edges dataframe\n",
    "G = nx.from_pandas_edgelist(edges, source = 'source', target = 'target', edge_attr = 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Graph the entire network.\n",
    "## This network is really just too big to easily visualize, so we will break it into communities later\n",
    "## This serves to show where we started from\n",
    "net=Network(height=\"100%\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\")\n",
    "net.from_nx(G)\n",
    "net.show_buttons(filter_=['physics'])\n",
    "net.show(\"Graphs/full_graph.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify communities\n",
    "There are many community recognition algorithms available with networkx.  \n",
    "I chose to go with Greed Modularity as I have the most experience with it.  \n",
    "Feel free to try out other algorithms!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gmc = greedy_modularity_communities(G, weight='weight', resolution=1)\n",
    "\n",
    "## This is how many communities were identified\n",
    "len(gmc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through the communities and graph each one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=1\n",
    "for comm in gmc:\n",
    "    temp_subgraph = G.subgraph(comm)\n",
    "    net=Network(height=\"100%\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\")\n",
    "    net.from_nx(temp_subgraph)\n",
    "\n",
    "    ## The below blocks can be turned on or off to adjust functionality in the graph\n",
    "    ## Turn this on to get a physics menu below the graph\n",
    "    ## Must turn off the set_options block below\n",
    "    #net.show_buttons(filter_=['physics'])\n",
    "    \n",
    "    ## Turn this on to get full control of the graph\n",
    "    ## Must turn off the set_options block below\n",
    "    #net.show_buttons()#(filter=['nodes, edges, layout, interaction, manipulation, physics, selection, renderer, physics'])\n",
    "\n",
    "    ## This sets the settings that are available in the commented out line above.\n",
    "    ## Remove this function if you want to start your settings from scratch.\n",
    "    net.set_options(\n",
    "        '''var options = {\n",
    "      \"nodes\": {\n",
    "        \"color\": {\n",
    "          \"border\": \"rgba(112,12,233,1)\",\n",
    "          \"background\": \"rgba(161,88,252,1)\",\n",
    "          \"highlight\": {\n",
    "            \"border\": \"rgba(17,229,233,1)\",\n",
    "            \"background\": \"rgba(100,255,254,1)\"\n",
    "          },\n",
    "          \"hover\": {\n",
    "            \"background\": \"rgba(170,201,255,1)\"\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"edges\": {\n",
    "        \"color\": {\n",
    "          \"color\": \"rgba(125,92,175,1)\",\n",
    "          \"highlight\": \"rgba(95,252,255,1)\",\n",
    "          \"hover\": \"rgba(102,103,255,1)\",\n",
    "          \"inherit\": false\n",
    "        },\n",
    "        \"smooth\": false\n",
    "      },\n",
    "      \"interaction\": {\n",
    "        \"hover\": true,\n",
    "        \"multiselect\": true\n",
    "      },\n",
    "      \"physics\": {\n",
    "        \"forceAtlas2Based\": {\n",
    "          \"gravitationalConstant\": -83,\n",
    "          \"springLength\": 135\n",
    "        },\n",
    "        \"minVelocity\": 0.75,\n",
    "        \"solver\": \"forceAtlas2Based\"\n",
    "      }\n",
    "    }'''\n",
    "    )\n",
    "\n",
    "    filename = 'Graphs/community_' + str(counter) + '_graph.html'\n",
    "    counter += 1\n",
    "    net.show(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optionally, select a specific community to graph\n",
    "- To do this just uncomment the blocks below\n",
    "- gmc is sorted by default with gmc[0] being the largest community and gmc[-1] being the smallest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm = gmc[0]\n",
    "subgraph=G.subgraph(comm)\n",
    "\n",
    "net=Network(height=\"100%\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\")\n",
    "net.from_nx(subgraph)\n",
    "\n",
    "## The below blocks can be turned on or off to adjust functionality in the graph\n",
    "## Turn this on to get a physics menu below the graph\n",
    "## Must turn off the set_options block below\n",
    "#net.show_buttons(filter_=['physics'])\n",
    "\n",
    "## Turn this on to get full control of the graph\n",
    "## Must turn off the set_options block below\n",
    "#net.show_buttons()#(filter=['nodes, edges, layout, interaction, manipulation, physics, selection, renderer, physics'])\n",
    "\n",
    "## This sets the settings that are available in the commented out line above.\n",
    "## Remove this function if you want to start your settings from scratch.\n",
    "net.set_options(\n",
    "    '''var options = {\n",
    "  \"nodes\": {\n",
    "    \"color\": {\n",
    "      \"border\": \"rgba(112,12,233,1)\",\n",
    "      \"background\": \"rgba(161,88,252,1)\",\n",
    "      \"highlight\": {\n",
    "        \"border\": \"rgba(17,229,233,1)\",\n",
    "        \"background\": \"rgba(100,255,254,1)\"\n",
    "      },\n",
    "      \"hover\": {\n",
    "        \"background\": \"rgba(170,201,255,1)\"\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"edges\": {\n",
    "    \"color\": {\n",
    "      \"color\": \"rgba(125,92,175,1)\",\n",
    "      \"highlight\": \"rgba(95,252,255,1)\",\n",
    "      \"hover\": \"rgba(102,103,255,1)\",\n",
    "      \"inherit\": false\n",
    "    },\n",
    "    \"smooth\": false\n",
    "  },\n",
    "  \"interaction\": {\n",
    "    \"hover\": true,\n",
    "    \"multiselect\": true\n",
    "  },\n",
    "  \"physics\": {\n",
    "    \"forceAtlas2Based\": {\n",
    "      \"gravitationalConstant\": -83,\n",
    "      \"springLength\": 135\n",
    "    },\n",
    "    \"minVelocity\": 0.75,\n",
    "    \"solver\": \"forceAtlas2Based\"\n",
    "  }\n",
    "}'''\n",
    ")\n",
    "\n",
    "\n",
    "net.show(\"Graphs/chosen_community_graph.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An example of a graph with all options enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm = gmc[0]\n",
    "subgraph=G.subgraph(comm)\n",
    "\n",
    "net=Network(height=\"100%\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\")\n",
    "net.from_nx(subgraph)\n",
    "\n",
    "net.show_buttons()\n",
    "\n",
    "net.show(\"Graphs/full_options_community_graph.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8bf545fee5244b87f8838516b4f261d3187914960dc9901da498b5b3355a3fd9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pythonProject')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
